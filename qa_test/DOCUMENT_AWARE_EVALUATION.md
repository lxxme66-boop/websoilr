# 文档感知问答对评测系统

## 概述

文档感知评测系统是对传统问答对质量评测的增强，它不仅评估问答对本身的质量，还评估问题从源文档中提取的合理性。这个系统能够：

1. **验证问答对与源文档的相关性** - 确保问题和答案确实基于文档内容
2. **评估提取的合理性** - 判断问题是否合理地从文档中提取
3. **检测无依据的问答对** - 识别没有文档支撑的问答内容
4. **分析问题的具体性** - 评估问题是否针对特定文档内容

## 核心功能

### 1. 文档加载与匹配
- 支持JSON和文本格式的文档
- 自动匹配问答对与源文档
- 使用TF-IDF相似度进行文档查找

### 2. 多维度评估
系统从以下维度评估提取合理性：

- **内容相关性 (25%)** - 问答对与文档的关键词重叠和语义相似度
- **答案依据性 (30%)** - 答案内容在文档中的支撑程度
- **问题具体性 (20%)** - 问题是否包含文档特定的实体或概念
- **信息完整性 (15%)** - 答案是否完整地回答了问题
- **LLM评估 (10%)** - 使用大语言模型进行综合评估

### 3. 问题识别
系统能够识别以下问题：
- 无文档支撑的问答对
- 过于通用的问题
- 答案与文档内容不符
- 提取不完整的信息

## 使用方法

### 基本用法

```bash
python evaluate_qa_with_documents.py \
    --input qa_pairs.json \
    --documents documents.json \
    --output results.json
```

### 参数说明

- `--input, -i`: 输入问答对文件（JSON格式）
- `--documents, -d`: 源文档文件（支持多个文件）
- `--output, -o`: 输出结果文件
- `--config, -c`: 配置文件路径（默认: config.yaml）
- `--min-reasonableness`: 最低合理性分数阈值（默认: 0.6）
- `--combine-evaluation`: 结合传统质量评估
- `--export-issues`: 导出有问题的问答对
- `--report-dir`: 报告输出目录

### 数据格式

#### 问答对格式
```json
{
  "question": "问题内容",
  "answer": "答案内容",
  "document_id": "doc_001",  // 可选：指定文档ID
  "source_document": "源文档内容",  // 可选：源文档片段
  "id": "qa_001",
  "metadata": {
    "source": "来源",
    "category": "分类"
  }
}
```

#### 文档格式
```json
{
  "id": "doc_001",
  "content": "文档内容",
  "metadata": {
    "source": "文档来源",
    "category": "文档分类",
    "date": "2024-01-01"
  }
}
```

## 评估流程

1. **文档加载**
   - 加载所有源文档
   - 构建TF-IDF向量索引

2. **文档匹配**
   - 根据document_id直接匹配
   - 使用相似度查找最相关文档
   - 记录未找到文档的问答对

3. **合理性评估**
   - 计算各维度分数
   - 加权计算总体合理性分数
   - 生成详细分析报告

4. **结果输出**
   - 筛选合理的问答对
   - 生成统计报告
   - 导出问题列表

## 配置选项

在 `config.yaml` 中配置文档感知评测：

```yaml
document_aware_evaluation:
  enabled: true
  use_llm_evaluation: true
  
  extraction_weights:
    content_relevance: 0.25
    answer_grounding: 0.30
    question_specificity: 0.20
    information_completeness: 0.15
    llm_evaluation: 0.10
  
  similarity_thresholds:
    document_match: 0.5
    fallback_match: 0.3
```

## 输出结果

### 评测结果文件
包含：
- 元数据（评测时间、文件信息等）
- 统计信息（总数、合理数、平均分等）
- 合理的问答对列表
- 详细的评分信息

### 评测报告
Markdown格式的详细报告，包含：
- 总体统计
- 分数分布
- 各维度分析
- 文档覆盖率
- 主要问题列表

### 问题导出
可选导出有问题的问答对，包含：
- 问答对ID和内容
- 合理性分数
- 主要问题类型

## 示例

运行示例脚本：

```bash
./document_aware_example.sh
```

这将演示：
1. 基本的文档感知评测
2. 综合评测（质量+合理性）
3. 多文档源评测
4. 报告生成和查看

## 最佳实践

1. **文档准备**
   - 确保文档内容完整且格式正确
   - 为文档分配唯一ID
   - 添加有意义的元数据

2. **问答对标注**
   - 尽可能提供document_id
   - 对于无文档支撑的问答对明确标注
   - 保持问答对与文档的一致性

3. **阈值调整**
   - 根据实际需求调整合理性阈值
   - 考虑不同类型问题的特点
   - 平衡准确性和召回率

4. **结果分析**
   - 重点关注低分问答对
   - 分析各维度得分分布
   - 根据反馈优化提取策略

## 扩展功能

系统支持以下扩展：
- 自定义评估维度和权重
- 集成更多文档格式
- 批量处理和并行评估
- 自定义问题类型分类
- 多语言支持

## 故障排除

常见问题：
1. **文档未找到** - 检查document_id是否正确
2. **相似度过低** - 调整相似度阈值或检查文档内容
3. **LLM评估失败** - 确保LLM服务正常运行
4. **内存不足** - 减少max_features或分批处理文档