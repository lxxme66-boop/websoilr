#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹é‡ç¿»è¯‘è¾…åŠ©å·¥å…·
æä¾›æ–‡ä»¶é¢„å¤„ç†ã€æ‰¹é‡ç®¡ç†ã€æ™ºèƒ½è°ƒåº¦ç­‰åŠŸèƒ½
"""

import os
import sys
import shutil
from pathlib import Path
import argparse
import json
from datetime import datetime

class TranslationHelper:
    def __init__(self):
        self.config_file = "translation_config.json"
        self.load_config()
    
    def load_config(self):
        """åŠ è½½é…ç½®"""
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        else:
            self.config = {
                "history": [],
                "preferences": {
                    "default_model": "2",  # 7Bæ¨¡å‹
                    "default_chunk_size": "2",  # ä¸­ç­‰ç‰‡æ®µ
                    "output_dir": "./chinese_translations"
                }
            }
    
    def save_config(self):
        """ä¿å­˜é…ç½®"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
    
    def preprocess_files(self, input_dir):
        """é¢„å¤„ç†æ–‡ä»¶"""
        print("ğŸ” é¢„å¤„ç†æ–‡ä»¶...")
        
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return
        
        txt_files = list(input_path.glob("*.txt"))
        
        if not txt_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(txt_files)} ä¸ªæ–‡ä»¶")
        
        # æ–‡ä»¶åˆ†æ
        file_stats = []
        total_size = 0
        encoding_issues = []
        
        for txt_file in txt_files:
            try:
                # æ£€æŸ¥ç¼–ç 
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    size = len(content)
                    lines = content.count('\n')
                    
                file_stats.append({
                    "name": txt_file.name,
                    "size": size,
                    "lines": lines,
                    "path": str(txt_file)
                })
                total_size += size
                
            except UnicodeDecodeError:
                encoding_issues.append(txt_file.name)
                # å°è¯•å…¶ä»–ç¼–ç 
                for encoding in ['gbk', 'gb2312', 'latin-1']:
                    try:
                        with open(txt_file, 'r', encoding=encoding) as f:
                            content = f.read()
                        # è½¬æ¢ä¸ºUTF-8
                        backup_path = txt_file.with_suffix('.bak')
                        shutil.copy(txt_file, backup_path)
                        with open(txt_file, 'w', encoding='utf-8') as f:
                            f.write(content)
                        print(f"âœ… è½¬æ¢ç¼–ç : {txt_file.name} ({encoding} â†’ UTF-8)")
                        break
                    except:
                        continue
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print("\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {len(txt_files)}")
        print(f"  æ€»å¤§å°: {total_size:,} å­—ç¬¦ ({total_size/1024/1024:.1f} MB)")
        print(f"  å¹³å‡å¤§å°: {total_size/len(txt_files):,.0f} å­—ç¬¦")
        
        if encoding_issues:
            print(f"\nâš ï¸  ç¼–ç é—®é¢˜æ–‡ä»¶: {len(encoding_issues)}ä¸ª")
            for f in encoding_issues[:5]:
                print(f"  - {f}")
        
        # æŒ‰å¤§å°åˆ†ç»„
        small_files = [f for f in file_stats if f['size'] < 10000]
        medium_files = [f for f in file_stats if 10000 <= f['size'] < 100000]
        large_files = [f for f in file_stats if f['size'] >= 100000]
        
        print("\nğŸ“ æ–‡ä»¶å¤§å°åˆ†å¸ƒ:")
        print(f"  å°æ–‡ä»¶ (<10Kå­—ç¬¦): {len(small_files)}ä¸ª")
        print(f"  ä¸­ç­‰æ–‡ä»¶ (10K-100K): {len(medium_files)}ä¸ª")
        print(f"  å¤§æ–‡ä»¶ (>100K): {len(large_files)}ä¸ª")
        
        # ç”Ÿæˆæ‰¹å¤„ç†å»ºè®®
        print("\nğŸ’¡ æ‰¹å¤„ç†å»ºè®®:")
        if large_files:
            print("  - å¤§æ–‡ä»¶å»ºè®®ä½¿ç”¨å°ç‰‡æ®µ(800å­—ç¬¦)ä»¥ç¡®ä¿å®Œæ•´ç¿»è¯‘")
        if len(txt_files) > 100:
            print("  - æ–‡ä»¶æ•°é‡è¾ƒå¤šï¼Œå»ºè®®ä½¿ç”¨åå°è¿è¡Œæˆ–screenä¼šè¯")
        if total_size > 10 * 1024 * 1024:  # 10MB
            print("  - æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨7Bæ¨¡å‹å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡")
        
        # ä¿å­˜æ–‡ä»¶åˆ—è¡¨
        file_list_path = "file_list.json"
        with open(file_list_path, 'w', encoding='utf-8') as f:
            json.dump(file_stats, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜åˆ°: {file_list_path}")
        
        return file_stats
    
    def split_batch(self, input_dir, batch_size=50):
        """å°†æ–‡ä»¶åˆ†æ‰¹å¤„ç†"""
        print(f"ğŸ“¦ åˆ†æ‰¹å¤„ç†æ–‡ä»¶ (æ¯æ‰¹{batch_size}ä¸ª)...")
        
        input_path = Path(input_dir)
        txt_files = list(input_path.glob("*.txt"))
        
        if not txt_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶")
            return
        
        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_dir = Path("translation_batches")
        batch_dir.mkdir(exist_ok=True)
        
        # æ¸…ç†æ—§æ‰¹æ¬¡
        for old_batch in batch_dir.glob("batch_*"):
            shutil.rmtree(old_batch)
        
        # åˆ†æ‰¹
        num_batches = (len(txt_files) + batch_size - 1) // batch_size
        
        for i in range(num_batches):
            batch_path = batch_dir / f"batch_{i+1:03d}"
            batch_path.mkdir(exist_ok=True)
            
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(txt_files))
            
            for j in range(start_idx, end_idx):
                # åˆ›å»ºç¬¦å·é“¾æ¥è€Œä¸æ˜¯å¤åˆ¶æ–‡ä»¶
                src = txt_files[j].absolute()
                dst = batch_path / txt_files[j].name
                if os.path.exists(dst):
                    os.remove(dst)
                os.symlink(src, dst)
            
            batch_files = end_idx - start_idx
            print(f"  æ‰¹æ¬¡ {i+1}: {batch_files} ä¸ªæ–‡ä»¶")
        
        print(f"\nâœ… å·²åˆ›å»º {num_batches} ä¸ªæ‰¹æ¬¡")
        print(f"ğŸ“ æ‰¹æ¬¡ç›®å½•: {batch_dir}")
        
        # ç”Ÿæˆæ‰¹å¤„ç†è„šæœ¬
        script_path = "run_batches.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write("#!/bin/bash\n\n")
            f.write("# æ‰¹é‡ç¿»è¯‘è„šæœ¬\n")
            f.write("# è‡ªåŠ¨å¤„ç†æ‰€æœ‰æ‰¹æ¬¡\n\n")
            
            for i in range(num_batches):
                f.write(f"echo 'å¤„ç†æ‰¹æ¬¡ {i+1}/{num_batches}...'\n")
                f.write(f"python translate_deepseek_improved.py < batch_{i+1}_input.txt\n")
                f.write(f"echo 'æ‰¹æ¬¡ {i+1} å®Œæˆ'\n")
                f.write("echo 'ç­‰å¾…10ç§’...'\n")
                f.write("sleep 10\n\n")
            
            f.write("echo 'âœ… æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ!'\n")
        
        os.chmod(script_path, 0o755)
        print(f"ğŸ’¾ æ‰¹å¤„ç†è„šæœ¬: {script_path}")
        
        # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºè¾“å…¥æ–‡ä»¶
        for i in range(num_batches):
            input_file = f"batch_{i+1}_input.txt"
            with open(input_file, 'w', encoding='utf-8') as f:
                # æ¨¡å‹é€‰æ‹©
                f.write(f"{self.config['preferences']['default_model']}\n")
                # è¾“å…¥ç›®å½•
                f.write(f"{batch_dir}/batch_{i+1:03d}\n")
                # è¾“å‡ºç›®å½•
                f.write(f"{self.config['preferences']['output_dir']}/batch_{i+1:03d}\n")
                # åˆ†ç‰‡å¤§å°
                f.write(f"{self.config['preferences']['default_chunk_size']}\n")
        
        return num_batches
    
    def merge_results(self, output_base_dir="./chinese_translations"):
        """åˆå¹¶æ‰¹æ¬¡ç»“æœ"""
        print("ğŸ”€ åˆå¹¶æ‰¹æ¬¡ç»“æœ...")
        
        output_path = Path(output_base_dir)
        batch_dirs = sorted(output_path.glob("batch_*"))
        
        if not batch_dirs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰¹æ¬¡ç›®å½•")
            return
        
        # åˆ›å»ºåˆå¹¶ç›®å½•
        merged_dir = output_path / "merged"
        merged_dir.mkdir(exist_ok=True)
        
        total_files = 0
        for batch_dir in batch_dirs:
            txt_files = list(batch_dir.glob("*_ä¸­æ–‡.txt"))
            for txt_file in txt_files:
                # ç§»åŠ¨åˆ°åˆå¹¶ç›®å½•
                dst = merged_dir / txt_file.name
                shutil.move(str(txt_file), str(dst))
                total_files += 1
        
        print(f"âœ… å·²åˆå¹¶ {total_files} ä¸ªæ–‡ä»¶åˆ°: {merged_dir}")
        
        # æ¸…ç†æ‰¹æ¬¡ç›®å½•
        for batch_dir in batch_dirs:
            if batch_dir.is_dir():
                shutil.rmtree(batch_dir)
        
        return total_files
    
    def create_report(self, input_dir, output_dir):
        """åˆ›å»ºç¿»è¯‘æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆç¿»è¯‘æŠ¥å‘Š...")
        
        report_path = "translation_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("DeepSeek ç¿»è¯‘æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¾“å…¥ç›®å½•: {input_dir}\n")
            f.write(f"è¾“å‡ºç›®å½•: {output_dir}\n\n")
            
            # è¿è¡Œè´¨é‡æ£€æŸ¥
            import subprocess
            result = subprocess.run(
                [sys.executable, "check_translation.py", input_dir, output_dir],
                capture_output=True,
                text=True
            )
            
            f.write(result.stdout)
            
            # æ·»åŠ å†å²è®°å½•
            self.config["history"].append({
                "time": datetime.now().isoformat(),
                "input_dir": input_dir,
                "output_dir": output_dir
            })
            self.save_config()
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡ç¿»è¯‘è¾…åŠ©å·¥å…·')
    parser.add_argument('action', choices=['preprocess', 'split', 'merge', 'report'],
                        help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--input-dir', '-i', help='è¾“å…¥ç›®å½•')
    parser.add_argument('--output-dir', '-o', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch-size', '-b', type=int, default=50,
                        help='æ¯æ‰¹æ–‡ä»¶æ•°é‡ (é»˜è®¤: 50)')
    
    args = parser.parse_args()
    
    helper = TranslationHelper()
    
    if args.action == 'preprocess':
        if not args.input_dir:
            print("âŒ è¯·æŒ‡å®šè¾“å…¥ç›®å½•")
            sys.exit(1)
        helper.preprocess_files(args.input_dir)
    
    elif args.action == 'split':
        if not args.input_dir:
            print("âŒ è¯·æŒ‡å®šè¾“å…¥ç›®å½•")
            sys.exit(1)
        helper.split_batch(args.input_dir, args.batch_size)
    
    elif args.action == 'merge':
        output_dir = args.output_dir or "./chinese_translations"
        helper.merge_results(output_dir)
    
    elif args.action == 'report':
        if not args.input_dir or not args.output_dir:
            print("âŒ è¯·æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºç›®å½•")
            sys.exit(1)
        helper.create_report(args.input_dir, args.output_dir)

if __name__ == "__main__":
    main()