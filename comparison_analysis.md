# 知识图谱构建器版本对比分析

## 1. 主要差异总结

### 1.1 核心技术差异

| 特性 | 原版 | 改进版 |
|------|------|--------|
| **实体识别方法** | jieba分词 + BERT嵌入 + 规则 | 大语言模型(LLM)直接抽取 |
| **关系抽取方法** | 规则匹配 + 触发词 | LLM统一抽取 |
| **模型类型** | BERT (AutoModel) | ChatGLM等生成式模型 (AutoModelForCausalLM) |
| **处理方式** | 逐句处理 | 文本分块处理 |
| **输出格式** | 程序内部构建 | LLM生成JSON后解析 |

### 1.2 输入输出一致性分析

#### 输入格式（✅ 一致）
两个版本的输入格式完全一致：
- 输入目录：包含 `*.txt` 文本文件的目录
- 配置格式：相同的配置字典结构

#### 输出格式（✅ 一致）
两个版本的输出格式基本一致：
- 都输出 NetworkX 的 MultiDiGraph 对象
- 都保存为相同格式的 JSON 文件，包含：
  - nodes: 节点列表（id, type, confidence等属性）
  - edges: 边列表（source, target, relation等属性）
  - statistics: 统计信息

#### 细微差异
改进版在输出中增加了一些字段：
- 节点增加了 `context`（实体所在句子）
- 节点增加了 `source_files`（来源文件列表）
- 边增加了 `evidence`（支持关系的句子）
- 统计信息增加了 `avg_degree`（平均度）

## 2. 大模型配置方法

### 2.1 配置结构
改进版通过配置字典传入大模型路径：

```python
config = {
    'models': {
        'llm_model': {
            'path': 'THUDM/chatglm3-6b'  # 模型路径或HuggingFace模型ID
        }
    },
    # 其他配置...
}
```

### 2.2 支持的模型类型
改进版支持任何 HuggingFace 上的生成式语言模型：

1. **ChatGLM系列**
   ```python
   'path': 'THUDM/chatglm3-6b'
   'path': 'THUDM/chatglm2-6b'
   ```

2. **Qwen系列**
   ```python
   'path': 'Qwen/Qwen-7B-Chat'
   'path': 'Qwen/Qwen-14B-Chat'
   ```

3. **Baichuan系列**
   ```python
   'path': 'baichuan-inc/Baichuan2-7B-Chat'
   'path': 'baichuan-inc/Baichuan2-13B-Chat'
   ```

4. **LLaMA系列**
   ```python
   'path': 'meta-llama/Llama-2-7b-chat-hf'
   'path': 'meta-llama/Llama-2-13b-chat-hf'
   ```

5. **本地模型**
   ```python
   'path': '/path/to/local/model'  # 本地模型目录
   ```

### 2.3 模型加载参数
改进版在加载模型时使用了优化参数：
- `torch_dtype=torch.float16`: 使用半精度减少显存占用
- `device_map="auto"`: 自动分配设备（支持多GPU）
- `trust_remote_code=True`: 允许执行模型仓库中的代码

## 3. 优缺点对比

### 原版优点
1. **轻量级**：使用BERT模型，资源占用少
2. **速度快**：基于规则的方法处理速度快
3. **可解释性强**：规则明确，易于调试

### 原版缺点
1. **准确率受限**：规则难以覆盖所有情况
2. **泛化能力弱**：新领域需要重新设计规则
3. **关系抽取简单**：主要依赖触发词

### 改进版优点
1. **准确率高**：LLM理解能力强
2. **泛化能力强**：适应新领域无需修改规则
3. **统一抽取**：实体和关系同时抽取，保持一致性
4. **上下文理解**：更好地理解长文本

### 改进版缺点
1. **资源占用大**：需要GPU和大量显存
2. **速度较慢**：LLM推理速度慢
3. **成本高**：计算成本高
4. **可能产生幻觉**：LLM可能生成不存在的实体

## 4. 使用建议

### 选择原版的场景
- 计算资源有限（无GPU或显存小）
- 对处理速度要求高
- 领域固定，规则明确
- 需要高度可控的结果

### 选择改进版的场景
- 有充足的GPU资源
- 追求更高的准确率
- 处理多样化的文本
- 需要理解复杂的语义关系

### 混合使用方案
可以考虑两种方法结合：
1. 先用原版快速处理，识别明显的实体
2. 对复杂或重要的文本使用改进版深度分析
3. 将两者结果合并，提高覆盖率

## 5. 模型选择建议

根据资源和需求选择合适的模型：

| 显存 | 推荐模型 | 特点 |
|------|----------|------|
| 8GB | ChatGLM3-6B | 中文优化，效果好 |
| 16GB | Qwen-7B-Chat | 综合能力强 |
| 24GB | Baichuan2-13B-Chat | 大模型，效果更好 |
| 32GB+ | Qwen-14B-Chat | 顶级效果 |

## 6. 性能优化建议

### 改进版优化
1. **批处理**：多个文本块同时处理
2. **缓存**：缓存已处理的结果
3. **量化**：使用4bit或8bit量化减少显存
4. **流式处理**：避免一次加载所有文本

### 原版优化
1. **并行处理**：多进程处理文件
2. **优化规则**：根据领域特点优化规则
3. **缓存词典**：预加载jieba词典