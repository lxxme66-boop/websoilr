#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ”¹è¿›ç‰ˆDeepSeekç¿»è¯‘å·¥å…·
- ä¿®å¤é•¿æ–‡æ¡£åªç¿»è¯‘ä¸€éƒ¨åˆ†çš„é—®é¢˜
- ä¼˜åŒ–æ–‡æœ¬åˆ†å‰²é€»è¾‘
- å¢åŠ è¿›åº¦æ˜¾ç¤ºå’Œé”™è¯¯å¤„ç†
"""

import os
import time
from pathlib import Path

def simple_translate():
    """ç®€åŒ–çš„ç¿»è¯‘æµç¨‹"""
    
    # ç›´æ¥æŒ‡å®šæ¨¡å‹è·¯å¾„
    model_paths = {
        "1": "/mnt/storage/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "2": "/mnt/storage/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", 
        "3": "/mnt/storage/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "4": "/mnt/storage/models/gte_Qwen2-7B-instruct"
    }
    
    print("ğŸš€ æ”¹è¿›ç‰ˆDeepSeekç¿»è¯‘å·¥å…·")
    print("=" * 50)
    
    print("å¯ç”¨æ¨¡å‹:")
    print("1. DeepSeek-R1-Distill-Qwen-1.5B (æœ€å¿«)")
    print("2. DeepSeek-R1-Distill-Qwen-7B (æ¨è)")
    print("3. DeepSeek-R1-Distill-Qwen-14B (é«˜è´¨é‡)")
    print("4. gte_Qwen2-7B-instruct (å¤‡é€‰)")
    
    # é€‰æ‹©æ¨¡å‹
    while True:
        choice = input("\nè¯·é€‰æ‹©æ¨¡å‹ (1-4): ").strip()
        if choice in model_paths:
            model_path = model_paths[choice]
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if os.path.exists(model_path):
                print(f"âœ… é€‰æ‹©æ¨¡å‹: {Path(model_path).name}")
                break
            else:
                print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                continue
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    input_dir = input("\nğŸ“ è¾“å…¥è‹±æ–‡txtæ–‡ä»¶å¤¹è·¯å¾„: ").strip()
    if not input_dir or not Path(input_dir).exists():
        print("âŒ è¾“å…¥è·¯å¾„æ— æ•ˆ")
        return
    
    output_dir = input("ğŸ“‚ è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ [é»˜è®¤: ./chinese_translations]: ").strip()
    if not output_dir:
        output_dir = "./chinese_translations"
    
    # è¯¢é—®åˆ†ç‰‡å¤§å°
    print("\næ–‡æœ¬åˆ†ç‰‡è®¾ç½®:")
    print("1. å°ç‰‡æ®µ (800å­—ç¬¦) - æ›´ç²¾ç¡®ä½†é€Ÿåº¦æ…¢")
    print("2. ä¸­ç‰‡æ®µ (1500å­—ç¬¦) - å¹³è¡¡é€‰æ‹©")
    print("3. å¤§ç‰‡æ®µ (3000å­—ç¬¦) - é€Ÿåº¦å¿«ä½†å¯èƒ½é—æ¼")
    chunk_choice = input("é€‰æ‹©åˆ†ç‰‡å¤§å° (1-3) [é»˜è®¤: 2]: ").strip()
    
    chunk_sizes = {"1": 800, "2": 1500, "3": 3000}
    chunk_size = chunk_sizes.get(chunk_choice, 1500)
    
    print(f"\nå¼€å§‹åŠ è½½æ¨¡å‹: {Path(model_path).name}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        # åŠ è½½æ¨¡å‹
        print("ğŸ”„ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # å¼€å§‹ç¿»è¯‘
        translate_files(input_dir, output_dir, model, tokenizer, chunk_size)
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

def translate_files(input_dir, output_dir, model, tokenizer, chunk_size=1500):
    """ç¿»è¯‘æ–‡ä»¶"""
    import torch
    
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    txt_files = list(input_path.glob("*.txt"))
    
    if not txt_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(txt_files)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ ä½¿ç”¨åˆ†ç‰‡å¤§å°: {chunk_size} å­—ç¬¦")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_chars = 0
    total_chunks = 0
    start_time = time.time()
    
    for i, txt_file in enumerate(txt_files, 1):
        print(f"\n[{i}/{len(txt_files)}] ç¿»è¯‘: {txt_file.name}")
        
        try:
            # è¯»å–æ–‡ä»¶
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                print("âš  æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            file_size = len(content)
            total_chars += file_size
            print(f"  æ–‡ä»¶å¤§å°: {file_size:,} å­—ç¬¦")
            
            # æ™ºèƒ½åˆ†å‰²æ–‡æœ¬
            chunks = smart_split_content(content, chunk_size)
            total_chunks += len(chunks)
            print(f"  åˆ†æˆ {len(chunks)} ä¸ªç‰‡æ®µ")
            
            translated_chunks = []
            
            for j, chunk in enumerate(chunks):
                print(f"  ç¿»è¯‘ç‰‡æ®µ {j+1}/{len(chunks)} ({len(chunk)} å­—ç¬¦)", end="")
                
                # æ„å»ºprompt
                prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹å®Œæ•´ç¿»è¯‘æˆä¸­æ–‡ã€‚æ³¨æ„ï¼š
1. ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œç»“æ„
2. ä¸“ä¸šæœ¯è¯­è¦å‡†ç¡®
3. ä¸è¦é—æ¼ä»»ä½•å†…å®¹

è‹±æ–‡åŸæ–‡ï¼š
{chunk}

ä¸­æ–‡ç¿»è¯‘ï¼š"""
                
                # ç”Ÿæˆç¿»è¯‘
                inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=4096)
                
                if inputs.shape[1] > 4096:
                    print(" âš  è¾“å…¥è¿‡é•¿ï¼Œè‡ªåŠ¨æˆªæ–­")
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs.to(model.device),
                        max_new_tokens=3000,  # å¢åŠ è¾“å‡ºé•¿åº¦
                        temperature=0.3,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # æå–ç¿»è¯‘
                if "ä¸­æ–‡ç¿»è¯‘ï¼š" in response:
                    translation = response.split("ä¸­æ–‡ç¿»è¯‘ï¼š")[-1].strip()
                else:
                    # å°è¯•å…¶ä»–åˆ†å‰²æ–¹å¼
                    translation = response[len(prompt):].strip()
                
                # ç¡®ä¿ç¿»è¯‘ä¸ä¸ºç©º
                if not translation:
                    print(" âš  ç¿»è¯‘ä¸ºç©ºï¼Œé‡è¯•")
                    translation = chunk  # ä¿ç•™åŸæ–‡
                
                translated_chunks.append(translation)
                print(" âœ“")
            
            # åˆå¹¶ç¿»è¯‘ç»“æœ
            result = "\n\n".join(translated_chunks)
            
            # æ·»åŠ å…ƒä¿¡æ¯
            header = f"""# {txt_file.name} - ä¸­æ–‡ç¿»è¯‘
# åŸæ–‡ä»¶å¤§å°: {file_size:,} å­—ç¬¦
# åˆ†ç‰‡æ•°é‡: {len(chunks)}
# ç¿»è¯‘æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
# ä½¿ç”¨æ¨¡å‹: {Path(model.name_or_path).name}

"""
            
            # ä¿å­˜ç»“æœ
            output_file = output_path / f"{txt_file.stem}_ä¸­æ–‡.txt"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(header + result)
            
            # éªŒè¯ç¿»è¯‘å®Œæ•´æ€§
            result_size = len(result)
            coverage = (result_size / file_size) * 100
            
            print(f"âœ… å®Œæˆ: {output_file.name}")
            print(f"  ç¿»è¯‘è¦†ç›–ç‡: {coverage:.1f}% ({result_size:,}/{file_size:,} å­—ç¬¦)")
            
            if coverage < 50:
                print("  âš ï¸ è­¦å‘Šï¼šç¿»è¯‘è¦†ç›–ç‡è¾ƒä½ï¼Œå¯èƒ½æœ‰å†…å®¹é—æ¼")
            
        except Exception as e:
            print(f"\nâŒ ç¿»è¯‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ€»ç»“
    elapsed = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… ç¿»è¯‘å®Œæˆ!")
    print(f"  æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"  æ€»ç‰‡æ®µæ•°: {total_chunks}")
    print(f"  æ€»ç”¨æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"  å¹³å‡é€Ÿåº¦: {total_chars/elapsed:.0f} å­—ç¬¦/ç§’")

def smart_split_content(text, max_length):
    """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œç¡®ä¿ä¸é—æ¼å†…å®¹"""
    
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    
    # é¦–å…ˆå°è¯•æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    current_chunk = ""
    
    for para in paragraphs:
        # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
        if len(para) > max_length:
            # å…ˆä¿å­˜å½“å‰å—
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = ""
            
            # åˆ†å‰²é•¿æ®µè½
            # æŒ‰å¥å­åˆ†å‰²
            sentences = split_into_sentences(para)
            
            for sent in sentences:
                if len(current_chunk) + len(sent) + 1 <= max_length:
                    current_chunk += (" " if current_chunk else "") + sent
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = sent
        
        else:
            # æ­£å¸¸æ®µè½
            if len(current_chunk) + len(para) + 2 <= max_length:
                current_chunk += ("\n\n" if current_chunk else "") + para
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = para
    
    # ä¸è¦å¿˜è®°æœ€åä¸€å—
    if current_chunk:
        chunks.append(current_chunk)
    
    # éªŒè¯æ²¡æœ‰å†…å®¹ä¸¢å¤±
    total_length = sum(len(chunk) for chunk in chunks)
    original_length = len(text)
    
    if abs(total_length - original_length) > 100:  # å…è®¸å°‘é‡å·®å¼‚ï¼ˆç©ºæ ¼ç­‰ï¼‰
        print(f"\nâš ï¸ è­¦å‘Šï¼šåˆ†å‰²åæ€»é•¿åº¦({total_length})ä¸åŸæ–‡({original_length})ç›¸å·®è¾ƒå¤§")
    
    return chunks

def split_into_sentences(text):
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    import re
    
    # ç®€å•çš„å¥å­åˆ†å‰²è§„åˆ™
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    # å¤„ç†è¿‡çŸ­çš„å¥å­ï¼ˆå¯èƒ½æ˜¯ç¼©å†™ç­‰ï¼‰
    result = []
    current = ""
    
    for sent in sentences:
        if len(sent) < 20 and current:  # å¤ªçŸ­çš„å¥å­åˆå¹¶
            current += " " + sent
        else:
            if current:
                result.append(current)
            current = sent
    
    if current:
        result.append(current)
    
    return result

if __name__ == "__main__":
    simple_translate()